defaults:
  - _default_optimization_attack
  - _self_
type: invertinggradients_diffusion
attack_type: invertinggradients_diffusion

objective:
  type: cosine-similarity
  scale: 1.0 # need to have a much smaller scale like 0.0001 for euclidean objectives

restarts:
  num_trials: 1
  scoring: "cosine-similarity"

init: randn
optim:
  optimizer: adam
  signed: "hard"
  step_size: 0.1
  boxed: False
  # max_iterations: 24_000
  max_iterations: 100
  # step_size_decay: step-lr
  step_size_decay: cosine-decay
  patched: 

  callback: 1000 # Print objective value every callback many iterations
  # callback: 100 # Print objective value every callback many iterations

regularization:
  total_variation:
    scale: 0.2
    # 0.2 # The old version did not take the mean dx + dy as the new version, so this corresponds to 0.1 in the old repo
    inner_exp: 1
    outer_exp: 1

  # channelgrad:
  #   scale: 0.3

accelerate:
  add_noise:
    # interval: 1000
    # scale: 0.01
    # num_levels: 20

  denoise: False

  hierarchical_gradient: True
# out_dir: "/home/zx/Gitrepo/breaching/out/patch_benchmark/invertgrad/origin_"
save:
  out_dir: "/home/zx/data/GitRepo/breaching/out/diffusion" 
  idx:
layer_weights: "equal"
  

diffusion:

  #sample
  clip_denoised: True
  num_samples: 1
  batch_size: 1
  # model_path: "/home/zx/data/GitRepo/breaching/breaching/attacks/accelarate/guided_diffusion/models/256x256_diffusion.pt"
  model_path: "/home/zx/data/GitRepo/breaching/breaching/attacks/accelarate/guided_diffusion/models/256x256_diffusion_uncond.pt"
    
  class_cond: False
  # class_cond: True

  # use_ddim: True
  # timestep_respacing: "ddim50"

  use_ddim: False
  timestep_respacing: "100"

  
  #classifier
  classifier_path: "/home/zx/data/GitRepo/breaching/breaching/attacks/accelarate/guided_diffusion/models/256x256_classifier.pt"
  classifier_scale: 10.0

  gradient_attack_scale: 1000.0
    
  classifier_use_fp16: False
  classifier_width: 128
  classifier_depth: 2
  classifier_attention_resolutions: "32,16,8"
  classifier_use_scale_shift_norm: True
  classifier_resblock_updown: True
  classifier_pool: "attention"
  
  #model
  image_size: 256
  num_channels: 256
  num_res_blocks: 2
  num_heads: 4
  num_heads_upsample: -1
  num_head_channels: 64
  attention_resolutions: "32,16,8"
  channel_mult: ''
  dropout: 0.0


  use_checkpoint: False
  use_scale_shift_norm: True
  resblock_updown: True
  use_fp16: True
  use_new_attention_order: False
  learn_sigma: True
  diffusion_steps: 1000
  noise_schedule: "linear"
  use_kl: False
  predict_xstart: False
  rescale_timesteps: False
  rescale_learned_sigmas: False
    